[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The Education \nAfter getting my Associate’s Degree in Mathematics from community college, I discovered the field of data science at UCSB. I quickly grew to love the field, and was ever enamored by the insight our data is capable of telling us. Along the way, I also learned a lot about the ethics of data science, and importance of unbiased data at the point of collection. I took both of these pieces with me as a fellow for the Central Coast Data Science Fellowship (CCDSP). I sat on the outreach committee, where I got to share my new found love of data science with high school students who were not privy to this powerful field. In my final year of my undergrad, I worked with the Energy and Environment Transitions lab on campus (ENVENT) for my capstone project. A team of three undergrads and myself were tasked with analyzing how demographic variables are differentially predictive of climate change opinions, using global survey data. It was during this project that I finally discovered what I wanted to be when I grow up.\n\n\nThe internship \nImmediately after graduating from the University of California, Santa Barbara, with a Bachelor of Science in Statistics and Data Science and a minor in Feminist Studies, I took on an internship with California Cooperative Oceanic Fisheries Investigations (CalCOFI). My role in this internship was to create an interactive data storytelling tool that would inform California Coastal residents on the threat of hypoxia in our California Coastal Ecosystem. The project stemmed from the realization that California Coastal residents are not widely informed about what hypoxia and how it changes with climate change. The project heavily relied on science communication to create an interactive web based primer through “scrollytelling.” Upon finishing the primer, another intern and I started an outreach portion of the internship where we went to local high schools to inform students on the field of environmental data science. We created an activity to help the students get their hands dirty analyzing real life environmental data collected by CalCOFI!\n\n\nThe life outside of my computer! \nI love to code, analyze, make cool visualizations, and learn about subjects I know little about through data science/analysis. But this isn’t all I do! I am a Southern California native and I take full advantage of it! I grew up mountain biking and hiking and have taken part in those activities ever since! I recently cheated on my mountain bike by purchasing a gravel bike and have since becomed hooked on road cycling and bikepacking. I have lived in Ventura County my whole life but moved to Ventura proper in 2022 and haven’t stopped biking my way between here and Santa Barbara since - the views never get old!\nOver the past year, I have really picked up an interest in zero waste cooking/cooking in general. I try and make a new semi- elaborate recipe every week. I make all my own almond and oat mylk, and repurpose the almond/oat pulp to make meal, crackers, and granola (one day I want to make my own coffee cart and sell matcha lattes using my own homeade nut milks as a passion project!) ! Living in an apartment without a garden, I don’t have a way to effectively compost. Instead, I freeze all my vegetable scraps and make vegetable broth each week with them. I am planning to start dabbling in bread making soon!"
  },
  {
    "objectID": "posts/bike_infographic/index.html",
    "href": "posts/bike_infographic/index.html",
    "title": "Seattle Bike Trends",
    "section": "",
    "text": "Below, I created an infographic to inform viewers on how bike usage changes seasonally and annually in Seattle, Washington. My goals of this infographic were to quantify the general number of bikes on the road in Seattle, compare bike usage across six different bike counting locations, and look into seasonal biking trends. In order to do this, I utilized data from Data.Seattle.gov. 1 I specifically utilized 6 different data sets that are all analogous in structure, but each for a different location. Each dataset contains four columns: the date, the number of bikes counted that were travelling northbound, the number of bikes counted that were travelling southbound, and the total number of bikes counted (the northbound column + the southbound column). The date column includes hourly data, so each observation in the dataset is a date and an individual hour. When I aggregated the six different locations into one data set, I added a column to specify the location, and removed the north and south bound columns.\nTo achieve the goals stated above, I created three different visualizations to include in my infographic. I first created a bar plot to show the total number of bikes counted in my aggregated dataset for each year (2014-2020). I also created an area graph that shows the total number of bikes counted monthly at each of the six different locations. Unlike the bar plot, this graph takes month and location into consideration. The third plot I created was a heatmap that aims to look at trends both monthly and yearly. I hoped that this plot could reveal any potential global warming trends- i.e. did biking start to become less popular in June as years progressed?\nWhen creating these plots, I made many different considerations to implement my design. The first decision I made was which types of plots I would use to convey each of my goals. I decided on a bar plot for the yearly total number of bikes counted because it easily conveyed the differences among the years. I used an area plot specifically for the monthly data as I wanted to visualize how usage varies from location to location. This graph also allowed me to see if certain locations were more popular in certain months than others- for example, were bike trails more popular in summer than a busy street? Finally, I decided on a heatmap to visualize how bike usage changed across both month and year because the different tiles make the count of bikes digestable with a continuous color scale. On all these plots, I made updates to the plot titles. In the case of the bar plot, I removed the title all together and utilized annotations instead. To remove extra clutter, I removed every other x axis tick mark (month) in the area plot. I updated the font for all plots to be consistent with the infographic title font. I made these updates in the theme() layer of each plot. Other updates to the theme() layer include adjusting the margins of the legend, rotating the legend (for the heatmap), increasing legend size (for the heatmap and area plot), and updating the size of x and y axis tick marks/ labels (all three plots).\nWhen arranging the elements in my infographic, I considered many different options to get my message across in a clean and concise way that was not visually overwhelming. I wanted my area plot and bar plot to be near each other, towards the bottom of the plot. I wanted these two plots to represent land type objects (the area plot a mountain, and the bar plot to be city buildings). I wanted them towards the bottom so they looked more grounded and didn’t represent a floating mountain/ buildings. Because of this, I placed the heatmap toward the top of the map. To eliminate too much text and blank space, I also added a space needle building image. I broke up the text into smaller chunks and spread them out to avoid paragraphs of text. In order to contextualize my data, I added a paragraph that provided a bit of background on the data itself and when it started to be collected. I also provided average annual and monthly temperatures via text boxes to allow viewers to contextualize what the temperatures might be when they see peaks and valleys of bike counts, both yearly and monthly. The central message that I wanted to get across was that time of year/ temperature played a large role in the number of bikers that were counted. I made this the central message not only through my plots, but also through the text and annotations that incorporated temperature in relation to each of the three plots.\nOnce I decided on the current color palette, I used color blind simulator to see if the palette I chose was color friendly. The color palette I chose was centered around differentiating the different locations in my area plot, and these differences were still clear among all color deficiencies. Before creating my infographic, I used a DEI lens by looking into the different locations that the bike sensors were implemented to see if they were in only wealthy neighborhoods that would likely count more bikes. The bike locations came from different road types (i.e. bike pathways, public roads, etc.) and were not privy to any certain area.\nI have included all code to my three different visualizations below! All plots were generated using ggplot() and I used magick() to add these plots together create a base infographic. When utilizing magick(), some text was rendering fuzzy, despite being clear in the individual PNGs.To make text as legible as possible, I ended up individually adding the PNGs to a Canva document. I used the base infographic that I created below (with the background color and title) as the background, and then added all other elements on top in Canva. I hope you enjoy exploring my infographic and code below!"
  },
  {
    "objectID": "posts/bike_infographic/index.html#infographic-overview",
    "href": "posts/bike_infographic/index.html#infographic-overview",
    "title": "Seattle Bike Trends",
    "section": "",
    "text": "Below, I created an infographic to inform viewers on how bike usage changes seasonally and annually in Seattle, Washington. My goals of this infographic were to quantify the general number of bikes on the road in Seattle, compare bike usage across six different bike counting locations, and look into seasonal biking trends. In order to do this, I utilized data from Data.Seattle.gov. 1 I specifically utilized 6 different data sets that are all analogous in structure, but each for a different location. Each dataset contains four columns: the date, the number of bikes counted that were travelling northbound, the number of bikes counted that were travelling southbound, and the total number of bikes counted (the northbound column + the southbound column). The date column includes hourly data, so each observation in the dataset is a date and an individual hour. When I aggregated the six different locations into one data set, I added a column to specify the location, and removed the north and south bound columns.\nTo achieve the goals stated above, I created three different visualizations to include in my infographic. I first created a bar plot to show the total number of bikes counted in my aggregated dataset for each year (2014-2020). I also created an area graph that shows the total number of bikes counted monthly at each of the six different locations. Unlike the bar plot, this graph takes month and location into consideration. The third plot I created was a heatmap that aims to look at trends both monthly and yearly. I hoped that this plot could reveal any potential global warming trends- i.e. did biking start to become less popular in June as years progressed?\nWhen creating these plots, I made many different considerations to implement my design. The first decision I made was which types of plots I would use to convey each of my goals. I decided on a bar plot for the yearly total number of bikes counted because it easily conveyed the differences among the years. I used an area plot specifically for the monthly data as I wanted to visualize how usage varies from location to location. This graph also allowed me to see if certain locations were more popular in certain months than others- for example, were bike trails more popular in summer than a busy street? Finally, I decided on a heatmap to visualize how bike usage changed across both month and year because the different tiles make the count of bikes digestable with a continuous color scale. On all these plots, I made updates to the plot titles. In the case of the bar plot, I removed the title all together and utilized annotations instead. To remove extra clutter, I removed every other x axis tick mark (month) in the area plot. I updated the font for all plots to be consistent with the infographic title font. I made these updates in the theme() layer of each plot. Other updates to the theme() layer include adjusting the margins of the legend, rotating the legend (for the heatmap), increasing legend size (for the heatmap and area plot), and updating the size of x and y axis tick marks/ labels (all three plots).\nWhen arranging the elements in my infographic, I considered many different options to get my message across in a clean and concise way that was not visually overwhelming. I wanted my area plot and bar plot to be near each other, towards the bottom of the plot. I wanted these two plots to represent land type objects (the area plot a mountain, and the bar plot to be city buildings). I wanted them towards the bottom so they looked more grounded and didn’t represent a floating mountain/ buildings. Because of this, I placed the heatmap toward the top of the map. To eliminate too much text and blank space, I also added a space needle building image. I broke up the text into smaller chunks and spread them out to avoid paragraphs of text. In order to contextualize my data, I added a paragraph that provided a bit of background on the data itself and when it started to be collected. I also provided average annual and monthly temperatures via text boxes to allow viewers to contextualize what the temperatures might be when they see peaks and valleys of bike counts, both yearly and monthly. The central message that I wanted to get across was that time of year/ temperature played a large role in the number of bikers that were counted. I made this the central message not only through my plots, but also through the text and annotations that incorporated temperature in relation to each of the three plots.\nOnce I decided on the current color palette, I used color blind simulator to see if the palette I chose was color friendly. The color palette I chose was centered around differentiating the different locations in my area plot, and these differences were still clear among all color deficiencies. Before creating my infographic, I used a DEI lens by looking into the different locations that the bike sensors were implemented to see if they were in only wealthy neighborhoods that would likely count more bikes. The bike locations came from different road types (i.e. bike pathways, public roads, etc.) and were not privy to any certain area.\nI have included all code to my three different visualizations below! All plots were generated using ggplot() and I used magick() to add these plots together create a base infographic. When utilizing magick(), some text was rendering fuzzy, despite being clear in the individual PNGs.To make text as legible as possible, I ended up individually adding the PNGs to a Canva document. I used the base infographic that I created below (with the background color and title) as the background, and then added all other elements on top in Canva. I hope you enjoy exploring my infographic and code below!"
  },
  {
    "objectID": "posts/bike_infographic/index.html#seattle-bike-count-infographic",
    "href": "posts/bike_infographic/index.html#seattle-bike-count-infographic",
    "title": "Seattle Bike Trends",
    "section": "Seattle Bike Count Infographic",
    "text": "Seattle Bike Count Infographic\n\n\nCode\nhtmltools::tags$iframe(style=\"width:100%; height:600px;\", src=\"output/infographic.pdf\")"
  },
  {
    "objectID": "posts/bike_infographic/index.html#code-for-infographic-elements",
    "href": "posts/bike_infographic/index.html#code-for-infographic-elements",
    "title": "Seattle Bike Trends",
    "section": "Code for Infographic Elements",
    "text": "Code for Infographic Elements\n\nNote that visualizations below are scaled to fit onto the infographic and not for stand alone purposes\n\n\nLoad Libraries\n\n\nCode\nlibrary(showtext)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(ggmap)\nlibrary(tmap)\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(ggimage)\nlibrary(patchwork)\nlibrary(ggtext)\nlibrary(magick)\nlibrary(png)\nlibrary(grid)\n\n\n\n\nImport and Wrangle Data\n\n\nCode\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                                import data                               ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\nbroadway&lt;- read.csv(here::here(\"posts\", \"bike_infographic\",\"raw-data\",\"Broadway_Cycle_Track_North_Of_E_Union_St_Bicycle_Counter__Out_of_Service__20240202.csv\"))\nburke &lt;- read.csv(here::here(\"posts\", \"bike_infographic\",\"raw-data\",\"Burke_Gilman_Trail_north_of_NE_70th_St_Bicycle_and_Pedestrian_Counter_20240202.csv\"))\nchief &lt;- read.csv(here::here(\"posts\", \"bike_infographic\",\"raw-data\",\"Chief_Sealth_Trail_North_of_Thistle_Bicycle_Counter__Out_of_Service__20240202.csv\"))\nelliott &lt;- read.csv(here::here(\"posts\", \"bike_infographic\",\"raw-data\",\"Elliott_Bay_Trail_in_Myrtle_Edwards_Park_Bicycle_and_Pedestrian_Counter__Out_of_Service__20240202.csv\"))\nmts &lt;- read.csv(here::here(\"posts\", \"bike_infographic\",\"raw-data\",\"MTS_Trail_west_of_I-90_Bridge_Bicycle_and_Pedestrian_Counter__Out_of_Service__20240202.csv\"))\nfifty_eight &lt;- read.csv(here::here(\"posts\", \"bike_infographic\",\"raw-data\",\"NW_58th_St_Greenway_at_22nd_Ave_NW_Bicycle_Counter__Out_of_Service__20240202.csv\"))\n\n\n\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                                merge data                            ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# rename total bike column for dataframes with bike only, create total bike column for dataframes with pedestrians ( since we dont want to include pedestrians in our dataframe)\n#do this renaming and totaling schema for all 7 datasets\n\nbroadway &lt;- broadway %&gt;% rename(\"bike_total\" = \"Broadway.Cycle.Track.North.Of.E.Union.St.Total\")\nburke$bike_total &lt;- burke$Bike.North + burke$Bike.South\nchief$bike_total &lt;- chief$Bike.North + chief$Bike.South\nelliott$bike_total &lt;- elliott$Bike.North + elliott$Bike.South\nfifty_eight &lt;- fifty_eight %&gt;% rename(\"bike_total\" = \"NW.58th.St.Greenway.st.22nd.Ave.NW.Total\")\nmts$bike_total &lt;- mts$Bike.East + mts$Bike.West\n\n\n#rename southbound and northbound columns to have consistent naming for all datasets\n#add locations column for each dataset with a string of what the location is \n#do this renaming and adding column step for all 7 datasets\nburke_clean &lt;- burke %&gt;% \n  rename(\"SB\" = \"Bike.South\" , \"NB\" = \"Bike.North\") %&gt;% \n  select(Date, bike_total, NB, SB, ) %&gt;% \n  mutate(loc = \"Burke\")\n\nchief_clean &lt;- chief %&gt;% \n  rename(\"SB\" = \"Bike.South\" , \"NB\" = \"Bike.North\") %&gt;% \n  select(Date, bike_total, NB, SB, ) %&gt;% \n  mutate(loc = \"Chief\")\n\nelliott_clean &lt;- elliott %&gt;% \n  rename(\"SB\" = \"Bike.South\" , \"NB\" = \"Bike.North\") %&gt;% \n  select(Date, bike_total, NB, SB, ) %&gt;% \n  mutate(loc = \"Elliott\")\n\nfifty_eight_clean &lt;- fifty_eight %&gt;% \n  rename (\"EB\" = \"East\", \"WB\" = \"West\") %&gt;% \n  select(Date, bike_total, EB, WB) %&gt;% \n  mutate(loc = \"58th\")\n\n  \n\nmts_clean &lt;- mts %&gt;% \n  rename(\"WB\" = \"Bike.West\" , \"EB\" = \"Bike.East\") %&gt;% \n  select(Date, bike_total, WB, EB, ) %&gt;% \n  mutate(loc = \"MTS Trail\")\n\nbroadway_clean &lt;- broadway %&gt;% \n  mutate(loc = \"Broadway\")\n\n\n#merge all cleaned dataframes that track north and south traffic\nbike_data &lt;- bind_rows(broadway_clean, burke_clean, chief_clean, elliott_clean,  mts_clean, fifty_eight_clean)\n\n\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n##                               create filtered dataframes                           ----\n##~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n# update bike data ( base data frame) date column to be in correct format, add year and month column for future filtering ------\n\n#update Date column to type POSIXct for future wrangling, format = current way date column is formatted\nbike_data$Date &lt;- as.POSIXct(bike_data$Date, format = \"%m/%d/%Y %I:%M:%S %p\")\n\n# create year column with year\nbike_data$Year &lt;- year(bike_data$Date)\n\n#create month column with month\nbike_data$Month &lt;- month(bike_data$Date) \nyears &lt;- c(2014,2015,2016,2017,2018,2019,2020)\nbike &lt;- bike_data[bike_data$Year %in% years, ]\n#create a dataframe of daily with the date, location and sum of bike counts for that day (i.e. aggregate hourly counts to be daily)-----\n\nbike_data_daily &lt;- bike %&gt;%\n  #create date column that is aggregated by day\n  mutate(date = floor_date(Date, unit = \"day\")) %&gt;%\n  #group by date and location\n  group_by(date, loc) %&gt;%\n  #create new column that has the daily count of bikes at each location each day\n  summarize(daily_sum = sum(bike_total, na.rm = TRUE), .groups = 'drop') %&gt;%\n  #drop na values (0)\n  drop_na()\n\n#create a dataframe that aggregates the monthly bike counts across all locations, should have two columns only ( month, monthly_total)\n\nbike_data_monthly &lt;-bike %&gt;%\n  #group by month\n  group_by(Month) %&gt;%\n  #create new column that has the monthly count of bikes at all locations\n  summarize(monthly_total = sum(bike_total, na.rm = TRUE)) %&gt;%\n  #drop na values (0)\n  drop_na()\n\n# create data frame of monthly bike counts at each locations, should have three columns ( month, monthly total, location)\nbike_data_monthly_loc &lt;-bike %&gt;%\n  #group by month and location\n  group_by(Month, loc) %&gt;%\n  #create new column that has the monthly count of bikes for each location\n  summarize(monthly_total = sum(bike_total, na.rm = TRUE)) %&gt;%\n  #drop na values (0)\n  drop_na()\n\nbike_data_year_month&lt;- bike %&gt;% \n  group_by(Year, Month) %&gt;% \n  summarize(monthly_total = sum(bike_total, na.rm = TRUE)) %&gt;%\n  #drop na values (0)\n  drop_na()\n#make month a factor\nbike_data_year_month$Month &lt;- factor(month.abb[bike_data_year_month$Month], levels = month.abb)\n#make year a factor\nbike_data_year_month$Year &lt;- factor(bike_data_year_month$Year)\n\n\n  \n\n#create filtered dataframe for heatmap\nbike_data_yearly &lt;- bike %&gt;% \n  group_by(Year) %&gt;% \n  summarize(yearly_total = sum(bike_total, na.rm = TRUE)) # add yearly totals for every month/year\n\n\n\n\nAdd necessary fonts and icon files\n\n\nCode\n#add font awesome icons \nfont_add(family = \"fa-brands\",\n         regular = here::here(\"posts\", \"bike_infographic\",\"otfs\", \"Font Awesome 6 Brands-Regular-400.otf\"))\nfont_add(family = \"fa-regular\",\n         regular = here::here(\"posts\", \"bike_infographic\",\"otfs\", \"Font Awesome 6 Free-Regular-400.otf\")) \nfont_add(family = \"fa-solid\",\n         regular = here::here(\"posts\", \"bike_infographic\",\"otfs\", \"Font Awesome 6 Free-Solid-900.otf\"))\n\n\n#..........................import fonts..........................\n# `name` is the name of the font as it appears in Google Fonts\n# `family` is the user-specified id that you'll use to apply a font in your ggpplot\n#add montserrat font\nfont_add_google(name = \"Montserrat\", family = \"montserrat\")\n\n#................enable {showtext} for rendering.................\nshowtext_auto()\n\n\n\n\nCreate area plot of monthly bikes counts at different locations\n\n\nCode\n#create breaks and labels for x axis labeling\nmonth_breaks &lt;- 1:12 \nmonth_labels &lt;- c(\"Jan\", \"\", \"Mar\",\"\", \"May\", \"\", \"Jul\",\"\", \"Sep\",\"\", \"Nov\",\"\")\n#month_labels &lt;- c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")\n\n#create color palette\ncustom_colors &lt;- c( \n                   \"Broadway\" = \"#706513\",  \n                   \"Burke\" = \"#B57114\", \n                   \"Elliott\" = \"#962B09\",\n                   \"Chief\"= \"#F2C078\",\n                   \"MTS Trail\" = \"#C1DBB3\",\n                   \"58th\"= \"#3891A6\")  \n\n\n#create geom area plot to show how bike traffic changes seasonally across diff locations\n\n# add data, fill area by location of bike sensor\nmountain &lt;- ggplot(data = bike_data_monthly_loc,  aes(x = Month, y = monthly_total, fill = loc)) +\n  #decrease the opacity\n  geom_area(alpha = 0.6) +\n  #add  color palette defined above\n  scale_fill_manual(values = custom_colors, labels = c(\"58th St.\", \"Broadway St.\", \"Burke Gilman Trail\", \"Chief Sealth Trail\", \"Elliott Bay Trail\", \"MTS Trail\")) +\n  #add rotated upward bike image, play around with sizes to fit top ridge of graph\n  geom_image(y = 510000, x = 2.5, image = \"images/rotate_up_bike.png\", size = .2 ) +\n  #add rotated downward bike image, play around with sizes to fit top ridge of graph\n  geom_image(y = 750000, x = 9.5, image = \"images/rotate_down_bike.png\", size = .2 ) +\n#add title, subtitle, x and y axis labels, and legend title\n  labs(#title = \"Seasonal Changes see heavier\\nbike traffic in Spring & Summer\",\n       #subtitle = \"Data from 2014 - 2020\",\n       x = \"Month\",\n       y = \"Number of Bikers Counted\",\n       fill = \"Location\") +\n  theme_minimal() +\n  \n  #add values and labels to x axis \n  scale_x_continuous(breaks = month_breaks, labels = month_labels)+\n  #convert y axis labels to be a standard number ( including e before)\n    scale_y_continuous(labels = scales::comma)+\n  #update theme\n  theme(\n    #remove grid elements and background elements \n    panel.grid.major = element_blank(), \n    panel.grid.minor = element_blank(),\n    axis.text.x = element_text(angle = 45, hjust = 1, colour = \"#FFFDD0\", size = 28, margin = margin(t = .4)),\n    axis.text.y = element_text( colour = \"#FFFDD0\", size = 28),\n    axis.line = element_blank(), # Removes axis lines\n    axis.ticks = element_blank(), # Removes axis ticks\n   axis.title.x = element_blank(), # Removes x-axis title\n    axis.title.y = element_blank(), # Removes y-axis title\n   # make background transparent for infographic\n   panel.background = element_rect(fill = \"transparent\", colour = NA),\n    plot.background = element_rect(fill = \"transparent\", colour = NA),\n   #update font to montserrat\n    text = element_text(family = \"montserrat\") ,\n   #update legend margins to be closer to plot\n   legend.margin = margin(t = 0, r = 0, b = -5, l = -10, unit = \"pt\"),\n    legend.box.margin = margin(t = 0, r = -5, b = -25, l = -5, unit = \"pt\"),\n   #updaete space and size between legend elements\n    legend.spacing.x = unit(6, \"pt\"),\n    legend.spacing.y = unit(.75, \"cm\"),\n   legend.position = c(.8,1),\n    legend.key.size = unit(30, \"pt\"),\n    legend.justification = c(1, 1),\n    legend.text = element_text(size = 20, colour = \"#FFFDD0\"),\n   legend.title = element_text(colour = \"#FFFDD0\", size = 28))+ # update font size\n      guides(fill = guide_legend(byrow = TRUE))\n\n\n\n#save plot as png to add to infographic base using magick\npng('output/mountain.png',width = 22, height = 20, units = 'in', res = 300, bg = \"transparent\")\nprint(mountain)  # Ensure the plot is explicitly printed\ninvisible(dev.off())\n\nmountain\n\n\n\n\n\n\n\nCreate bar plot of total number of bikes counted per year\n\n\nCode\n#initiate ggplot with year\ncity_buildings &lt;- ggplot(bike_data_yearly, aes(x = Year, y = yearly_total)) +\n#add geom column layer, lower opacity, adjust width to represent buidlings, make bars creme with a think black outline\ngeom_col(width = .7, alpha = .7, fill = \"#FFFDD0\", color = \"black\", size = .2) +\n\n  #add year at the bottom of each bar in green\n geom_text(aes(label = Year, y = 0.05 * max(yearly_total)), size = 5, color = \"black\")+ \n\n  theme_minimal()+\n  #update theme\n  theme(\n    #remove grid elements and background elements\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.background = element_blank(),\n    plot.background = element_blank(),\n    axis.line = element_blank(), # Removes axis lines\n    axis.text.x = element_blank(), # Removes x-axis labels\n    axis.text.y = element_blank(), # Removes y-axis labels\n    axis.ticks = element_blank(), # Removes axis ticks\n    axis.title.x = element_blank(), # Removes x-axis title\n    axis.title.y = element_blank(), # Removes y-axis title\n    text = element_text(family = \"montserrat\")  ) # update font\n\n\n\n#save plot as png to add to infographic base using magick\n  ggsave('output/city_buildings.png', plot = city_buildings, device = 'png',width = 700, height = 800, units = 'px', dpi = 700)\n\ncity_buildings\n\n\n\n\n\n\n\nCreate heatmap to show how bike counts vary across both month and year\n\n\nCode\n#initiate ggplot with year and month, fill by monthly total\nheatmap &lt;- ggplot(bike_data_year_month, aes(x = Month, y = Year, fill = monthly_total)) +\n  geom_tile(color = \"white\") +  # Add white border to the tile\n  scale_fill_gradient(low = \"#FFFDD0\", high = \"#B57114\",  labels = scales::comma) +  # Define colors for the gradient\n  theme_minimal() +\n  scale_y_discrete(limits = rev(levels(bike_data_year_month$Year))) +  # Reverse order of years on y axis\n  #labs(title = \"Seasonal Changes see heavier\\nbike traffic in Spring & Summer\") +  # Add title\n   guides(\n     fill = guide_colourbar(title = \"Monthly Total\", title.position = \"bottom\", title.hjust = 0.5, barwidth = 5, barheight = .25,  # Adjust the size of the gradient bar and place legend underneath plot\n    label.position = \"bottom\")\n   ) + \n  theme(\n    #angle x axis ticks, update size and typeface color\n    axis.text.x = element_text(angle = 45, hjust = 1, colour = \"#FFFDD0\", size = 11),\n    #update size and typeface color for y axis ticks\n    axis.text.y = element_text(colour = \"#FFFDD0\", size = 11),  \n    #remove grid elements\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    # remove background by making transparent\n    panel.background = element_rect(fill = \"transparent\", colour = NA),\n    plot.background = element_rect(fill = \"transparent\", colour = NA),\n    #remove axis line and x and y axis labels\n    axis.line = element_blank(),\n    axis.title.x = element_blank(),\n    axis.title.y = element_blank(),\n    #update typface\n    text = element_text(family = \"montserrat\"),\n    #place legend at bottom\n    legend.position = \"bottom\", \n    #update legend marigins\n    legend.margin = margin(t = -5, r = -10, b = -5, l = -10, unit = \"pt\"),\n    #update legend box margins\n    legend.box.margin = margin(t = 0, r = 0, b = 0, l = 0, unit = \"pt\"),\n    #update spacing around x and y axis of legend\n    legend.spacing.x = unit(2, \"pt\"),\n    legend.spacing.y = unit(2, \"pt\"),\n    legend.key.size = unit(1, \"pt\"),\n    #update legend text size\n    legend.text = element_text(size = 10, colour = \"#FFFDD0\"),\n    #remove legend title\n    legend.title = element_blank(),\n    plot.title = element_text(colour = \"#FFFDD0\", hjust = 0.5, size = 14)\n  )\n\n\n#save plot as png to add to infographic base using magick\n ggsave('output/heatmap.png', plot = heatmap, device = 'png',width = 500, height = 500, units = 'px', dpi =300)\n\n\nheatmap\n\n\n\n\n\n\n\nInfographic Base\n\n\nCode\n#.........................create caption.........................\n\n#create bike color palette for future reference\nbike_colors &lt;- c(\"#706513\",    \"#B57114\",  \"#962B09\", \"#F2C078\",      \"#C1DBB3\",\"#3891A6\")\n#text color for infographic\ntxt &lt;- bike_colors[4]\n#background color for infographic\nbg &lt;- bike_colors[3]\n\n#initialize infographic background\ng_base &lt;- ggplot() +\n  labs(\n    title = \"Seattle Bike Trends\", # add title to infographic\n    subtitle = \"Bikes counts from six bike sensor stations from 2014 to 2020\", # add subtitle to infographic\n    ) +\n  theme_void() + \n  #update theme elements\n  theme(\n    # specify typeface and font size for title\n  text = element_text(family = \"montserrat\", size = 20, lineheight = 1.2, colour = txt),  \n  #fill infographic background with specified background color\n    plot.background = element_rect(fill = bg, colour = bg), \n  #make title bold, adjust margins and horizontal justification\n    plot.title = element_text(size = 35, face = \"bold\", hjust = 0.5, margin = margin(b = 20)),  \n  #update subtitle typeface, adjst margins and horizontal justification\n    plot.subtitle = element_text(family = \"montserrat\", hjust = 0.5, margin = margin( t = -20, b = 30), size = 12),  \n  #adjut plot margins\n    plot.margin = margin(b = 10, t = 7, r = 10, l = 10)  \n  )\n\ng_base\n\n\n\n\n\nCode\n#save plot as png to add to make infographic  using magick\n ggsave('output/g_base.png', plot = g_base, device = 'png',width = 675, height = 1200, units = 'px', dpi = 300)\n\n\n\n\nCode\n#read all images created above and remove edges that are the background color from the plots\nmountain_image &lt;- image_read('output/mountain.png') %&gt;%\n  image_trim() \n\nbuildings_image &lt;- image_read('output/city_buildings.png') %&gt;%\n  image_trim()\n\nheatmap_image &lt;- image_read('output/heatmap.png') %&gt;%\n  image_trim()\n\n\nspace_needle_image &lt;- image_read('output/SeattleSpaceNeedle.png')\n\n\ntitle_image &lt;- image_read('output/g_base.png')\n\n#arrange all plots onto infographic base\ntitle_image %&gt;%\n  # add mountain plot on the middle left of plot\n  image_composite(image_scale(mountain_image, '400x'), offset = '+2+450') %&gt;%  \n  #add the buildings image on the lower right\n  image_composite(image_scale(buildings_image, \"300x\"), gravity = \"Center\", offset = \"+180+425\") %&gt;%  \n  #add the heatmap on the top right\n  image_composite(image_scale(heatmap_image, \"350\"), offset = \"+280+125\") %&gt;%  \n  #add the space needle png next to the buildings\n   image_composite(image_scale(space_needle_image, '100x'), gravity = \"Center\", offset = '-10+444') %&gt;% \n#create png\n  image_write(path = \"output/postcard.png\")\n\n#assign png to an R variable\ninfographic_base &lt;- readPNG('output/postcard.png')\n#print infographic\ngrid.raster(infographic_base)"
  },
  {
    "objectID": "posts/bike_infographic/index.html#footnotes",
    "href": "posts/bike_infographic/index.html#footnotes",
    "title": "Seattle Bike Trends",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSeattle Department of Transportation. (2023). Bike Counters. https://www.seattle.gov/transportation/projects-and-programs/programs/bike-program/bike-counters↩︎"
  },
  {
    "objectID": "posts/aqi-santa-barbara/air_quality_index-sb.html",
    "href": "posts/aqi-santa-barbara/air_quality_index-sb.html",
    "title": "Analyzing air quality after the Santa Barbara Thomas Fire",
    "section": "",
    "text": "More information on this analysis can be found here\nBoth tabular and raster data allow us to see changes in data in different ways. Here, we will use an AQI tabular dataset that reports air quality levels in the US, along with landsat satelite data to see if air quality effects from the Thomas Fire can be seen by both numeric data as well as visual data.\n\nImage above shows dark clouds caused by the Thomas fire over the ocean in December 2017.\n\n\n\n\nThe Thomas Fire caused extensive damage in both Ventura and Santa Barbara County in December of 2017. As with all fires, the decrease in air quality index is a a direct effect of wildfires. This project will explore how the air quality index was effected by the Thomas fire. We will look into both numeric AQI data of santa barbara, as well as a collection of bands from the Landsat 8 Satellite. Looking into this different areas will allow us to further investigate how much aqi was affected, and from what bands the effect could be seen.\n\n\n\n\nfetch AQO data, landsat sattelite data, and california fire perimeter shapefile from an online repository\nClean data with consistent crs’ for plotting purposes\nTime series analysis with aqi data\nVisualize landsat raster data with false and true color images\nCompare landsat raster with Thomas fire perimeter\n\n\n\n\n\n\n\nA simplified collection of bands (red, green, blue, near-infrared and shortwave infrared) from the Landsat Collection 2 Level-2 atmosperically corrected surface reflectance data, collected by the Landsat 8 satellite. The data was accessed and pre-processed in the Microsoft Planetary Computer to remove data outside land and coarsen the spatial resolution. Data can be found in the Landsat Collection in the Microsoft Planetary Computer here.\n\n\n\nA shapefile of fire perimeters in California during 2017. The complete file can be accessed in the CA state geoportal.\n\n\n\nA daily reporting of AQI by county for 2017 and 2018. Data reports an AQI value, approximately every 3 -7 days, for counties in every state. Also reports the category for AQI, i.e. “good”. Data reports an AQI value, approximately every 3 -7 days, for 804 counties across every state (as well as Country of Mexico, Puerto Rico, and Virgin Islands).\n\n\n\nCalifornia State Geoportal. 2023. California Fire Perimeters (All). https://gis.data.ca.gov/datasets/CALFIRE-Forestry::california-fire-perimeters-all-1/about (Accessed 2023-11-28).\nNASA, USGS. 2023. Landsat Collection 2 Level-2. Hosted by Microsoft Planetary Computer.https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2 (Accessed 2023-11-22).\nUnited States Environmental Protection Agency. Daily AQI by County.https://aqs.epa.gov/aqsweb/airdata/download_files.html#AQI (Accessed 2023-11-22).\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport os\nimport matplotlib.patches as mpatches # for creating legends\nimport xarray as xr\nimport rioxarray as rioxr\nimport geopandas as gpd\nfrom rasterio.features import rasterize #for reasterizing polygons\nimport matplotlib.lines as mlines\n\n\n\n\nCode\n#read in aqi data for 2017 and 2018\naqi_17 = pd.read_csv('https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2017.zip') # 2017 data\naqi_18 = pd.read_csv('https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2018.zip') # 2018 data\n\n# read in shapefile of califronia fire/ perimters\nca_fire = gpd.read_file('data/California_Fire_Perimeters_2017/California_Fire_Perimeters_2017.shp') \n\n#read in landsat data\nlandsat_fp = os.path.join(os.getcwd(), 'data','landsat8-2018-01-26-sb-simplified.nc') \nlandsat = rioxr.open_rasterio(landsat_fp)\n\n\n\n\n\nSelect data pertaining to area of interest (Santa Barbara) and merge data for our years of interest ( 2017 and 2018)\n\n\nCode\n # concatenate 2017 and 2018 data using concat to get one dataframe for all aqi data\naqi = pd.concat([aqi_17, aqi_18])\n\n#only select data in Santa barbara since that is where Thomas fire was \naqi_sb = aqi[aqi[\"county Name\"] == \"Santa Barbara\"].drop(columns = ['State Name','county Name','State Code','County Code']) #select Santa barbara and drop unnecessary columns\n\n#only select thomas fire perimeter\nthomas_fire = ca_fire[ca_fire.FIRE_NAME == \"THOMAS\"]\n\n#check to make sure there is now only one unqiue value in thomas _fire\nprint(thomas_fire.FIRE_NAME .nunique())\n\n\n1\n\n\n\n\nCode\naqi.head()\n\n\n\n\n\n\n\n\n\nState Name\ncounty Name\nState Code\nCounty Code\nDate\nAQI\nCategory\nDefining Parameter\nDefining Site\nNumber of Sites Reporting\n\n\n\n\n0\nAlabama\nBaldwin\n1\n3\n2017-01-01\n21\nGood\nPM2.5\n01-003-0010\n1\n\n\n1\nAlabama\nBaldwin\n1\n3\n2017-01-04\n22\nGood\nPM2.5\n01-003-0010\n1\n\n\n2\nAlabama\nBaldwin\n1\n3\n2017-01-10\n19\nGood\nPM2.5\n01-003-0010\n1\n\n\n3\nAlabama\nBaldwin\n1\n3\n2017-01-13\n30\nGood\nPM2.5\n01-003-0010\n1\n\n\n4\nAlabama\nBaldwin\n1\n3\n2017-01-16\n16\nGood\nPM2.5\n01-003-0010\n1\n\n\n\n\n\n\n\n\n\n\nExplore information on all three datasets to understand data setup\n\n\n\n\nCode\nprint(\"Number of unique values : \\n\", aqi_sb.nunique(), \"\\n\") # check number of unique values to get idea on variety of data\nprint(\"Column datatypes:\\n \", aqi_sb.dtypes, \"\\n\")  # check data types of variables for future reference\nprint(\"Shape of data:\", aqi_sb.shape) # check shape of data to see how many columns and observations there are\n\n\nNumber of unique values : \n Date                         730\nAQI                           75\nCategory                       5\nDefining Parameter             3\nDefining Site                 12\nNumber of Sites Reporting      4\ndtype: int64 \n\nColumn datatypes:\n  Date                         object\nAQI                           int64\nCategory                     object\nDefining Parameter           object\nDefining Site                object\nNumber of Sites Reporting     int64\ndtype: object \n\nShape of data: (730, 6)\n\n\n\n\n\n\n\nCode\nprint(\"Column names: \\n\", ca_fire.columns) # check column names to see what the data contains\nprint(\"Shape of data:\", ca_fire.shape,\"\\n\") # check shape of data to see how many columns and observations there are\nprint(\"Column datatypes:\\n \", ca_fire.dtypes, \"\\n\")  # check data types of variables for future reference\n\n\nColumn names: \n Index(['index', 'OBJECTID', 'YEAR_', 'STATE', 'AGENCY', 'UNIT_ID', 'FIRE_NAME',\n       'INC_NUM', 'ALARM_DATE', 'CONT_DATE', 'CAUSE', 'C_METHOD', 'OBJECTIVE',\n       'GIS_ACRES', 'COMMENTS', 'COMPLEX_NA', 'COMPLEX_IN', 'IRWINID',\n       'FIRE_NUM', 'DECADES', 'SHAPE_Leng', 'SHAPE_Area', 'geometry'],\n      dtype='object')\nShape of data: (608, 23) \n\nColumn datatypes:\n  index            int64\nOBJECTID         int64\nYEAR_           object\nSTATE           object\nAGENCY          object\nUNIT_ID         object\nFIRE_NAME       object\nINC_NUM         object\nALARM_DATE      object\nCONT_DATE       object\nCAUSE          float64\nC_METHOD       float64\nOBJECTIVE      float64\nGIS_ACRES      float64\nCOMMENTS        object\nCOMPLEX_NA      object\nCOMPLEX_IN      object\nIRWINID         object\nFIRE_NUM        object\nDECADES         object\nSHAPE_Leng     float64\nSHAPE_Area     float64\ngeometry      geometry\ndtype: object \n\n\n\n\n\n\n\n\nCode\nprint(\"The dimmensions of the landsat data are:\\n\", landsat.dims,\"\\n\") # check file dimmensions\nprint(\"The coordinates of the landsat data are:\\n\", landsat.coords,\"\\n\") # check coordinates of shapefile\nprint(\"The values of the landsat data are:\\n\",  landsat.values, \"\\n\") # values of xarray\nprint(\"The crs of the landsat data is:\", landsat.rio.crs) # check crs\n\n\nThe dimmensions of the landsat data are:\n Frozen({'y': 731, 'x': 870, 'band': 1}) \n\nThe coordinates of the landsat data are:\n Coordinates:\n  * y            (y) float64 3.952e+06 3.952e+06 ... 3.756e+06 3.755e+06\n  * x            (x) float64 1.213e+05 1.216e+05 ... 3.557e+05 3.559e+05\n  * band         (band) int64 1\n    spatial_ref  int64 0 \n\nThe values of the landsat data are:\n &lt;bound method Mapping.values of &lt;xarray.Dataset&gt;\nDimensions:      (y: 731, x: 870, band: 1)\nCoordinates:\n  * y            (y) float64 3.952e+06 3.952e+06 ... 3.756e+06 3.755e+06\n  * x            (x) float64 1.213e+05 1.216e+05 ... 3.557e+05 3.559e+05\n  * band         (band) int64 1\n    spatial_ref  int64 0\nData variables:\n    red          (band, y, x) float64 ...\n    green        (band, y, x) float64 ...\n    blue         (band, y, x) float64 ...\n    nir08        (band, y, x) float64 ...\n    swir22       (band, y, x) float64 ...&gt; \n\nThe crs of the landsat data is: EPSG:32611\n\n\n\n\n\n\n\n\nAdjust column names, make data a datetime object, and set index of dataset to be data for future plotting purposes\n\n\nCode\naqi_sb.columns = aqi_sb.columns.str.lower() #make column names lower case\naqi_sb.columns = aqi_sb.columns.str.replace(' ','_') # reassign column names by substituting an underscore for a space\naqi_sb.date = pd.to_datetime(aqi_sb.date) # update data column to datetime\naqi_sb = aqi_sb.set_index('date') # change index to be date\n\n\n\n\n\nDrop Landsat band so we are able to plot landsat raster\n\n\nCode\n# drop band dimension to make data 2D\nlandsat_2d = landsat.squeeze().drop('band')\nlandsat_2d # check to make sure band is no longer included\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:      (y: 731, x: 870)\nCoordinates:\n  * y            (y) float64 3.952e+06 3.952e+06 ... 3.756e+06 3.755e+06\n  * x            (x) float64 1.213e+05 1.216e+05 ... 3.557e+05 3.559e+05\n    spatial_ref  int64 0\nData variables:\n    red          (y, x) float64 ...\n    green        (y, x) float64 ...\n    blue         (y, x) float64 ...\n    nir08        (y, x) float64 ...\n    swir22       (y, x) float64 ...xarray.DatasetDimensions:y: 731x: 870Coordinates: (3)y(y)float643.952e+06 3.952e+06 ... 3.755e+06axis :Ycrs :EPSG:32611long_name :y coordinate of projectionresolution :-30standard_name :projection_y_coordinateunits :metre_FillValue :nanarray([3952395., 3952125., 3951855., ..., 3755835., 3755565., 3755295.])x(x)float641.213e+05 1.216e+05 ... 3.559e+05axis :Xcrs :EPSG:32611long_name :x coordinate of projectionresolution :30standard_name :projection_x_coordinateunits :metre_FillValue :nanarray([121305., 121575., 121845., ..., 355395., 355665., 355935.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32611\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 11Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32611\"]]GeoTransform :121170.0 270.0 0.0 3952530.0 0.0 -270.0array(0)Data variables: (5)red(y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]green(y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]blue(y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]nir08(y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]swir22(y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]Attributes: (0)\n\n\n\n\n\nMake sure crs of thomas fire shapefile is same as the crs for the landsat raster, change crs if not the same.\n\n\nCode\nthomas_fire= thomas_fire.to_crs(landsat_2d.rio.crs) # change crs of thomas fire to match landsat crs\n\n\n\n\nCode\n#check to make sure crs' match/\nthomas_fire.crs == landsat.rio.crs\n\n\nTrue\n\n\n\n\n\n\n\n\nPlot five day average of aqi and daily aqi to see different trends\n\n\nCode\naqi_sb['five_day_average'] = aqi_sb.aqi.rolling('5D').mean() # create new column called 'five_day_average' in aqi_sb data frame that contains a 5 day rolling window\ncolors = {'aqi': '#964B00', # pick dark brown for aqi\n         'five_day_average': 'cyan'}  # pick cyan for five day average\naqi_sb.plot(use_index = True, # include index as x axis\n       y = ['aqi','five_day_average'], # plot aqi and five_day_average\n       color = colors,\n           title = 'AQI and Five day AQI Average in Santa Barbara County from Jan 2017 to December 2018') # have colors be pre defined colors dictionary, add title\n\n\n&lt;AxesSubplot:title={'center':'AQI and Five day AQI Average in Santa Barbara County from Jan 2017 to December 2018'}, xlabel='date'&gt;\n\n\n\n\n\n\n\n\nCreate a true color image by plotting the red, green, and blue variables.\n\n\nCode\nfig, ax = plt.subplots(figsize = (12,8)) # set figure size and axis\nax.axis('off') # turn x and y axis off\n\n# -------------------------\nlandsat_2d[['red', 'green', 'blue']].to_array().plot.imshow(ax = ax,robust = True)\nthomas_fire.plot(ax = ax,color = 'None', edgecolor = 'red') # plot california map with black outline \nthomas_line = mlines.Line2D([], [], color='red', marker='_', # make legend have triangle #update legend name and marker\n                          markersize=15, label='Thomas Fire Perimeter', linestyle = 'None')\n\n#---------------------------\n\n# create legend\nax.legend(handles = [thomas_line], frameon = True, loc = 'upper right') # add legend in upper right corner\n\n#---------------------------\nplt.title(\"True color image\")\n\nplt.show()  # show plot\n\n\n\n\n\nCreate a false color image by plotting the short-wave infrared (swir22), near-infrared, and red variables.\n\n\nCode\n#select short wave, near infrared, and red variables, make an array and plot with robust = True\nlandsat_2d[[ 'swir22','nir08', 'red']].to_array().plot.imshow(size = 4, robust = True)\nplt.title(\"False color image\")\n\n\nText(0.5, 1.0, 'False color image')\n\n\n\n\n\nThis image looks good, but we want to look closer at the Thomas fire region. We can zoom in on the Thomas fire region to see that the landsat imaage picked up for the region using a false color image.\n\n\nCode\nfig, ax = plt.subplots(figsize = (12,8)) # set figure size and axis\nax.axis('off') # turn x and y axis off\n\n# -------------------------\nlandsat_2d[['swir22','nir08', 'red']].to_array().plot.imshow(ax = ax,robust = True)\nthomas_fire.plot(ax = ax,color = 'None', edgecolor = 'red') # plot california map with red outline \nthomas_line = mlines.Line2D([], [], color='red', marker='_', #make legend symbol be a line and change color to red, add label\n                          markersize=15, label='Thomas Fire Perimeter', linestyle = 'None')\n\n#---------------------------\n\n# create legend\nax.legend(handles = [thomas_line], frameon = True, loc = 'upper right') # add legend in upper right corner\n#---------------------------\nplt.title(\"False color image with Thomas Fire Perimeter\")\n\nplt.show()  # show plot\n\n\n\n\n\n\n\n\n\nThe AQI time series graph and landsat image above both display increased AQI levels during the Thomas fire. The time series graph has a spike in AQI during December of 2017/ January 2018. This corresponds to the time of the Thomas Fire, when we would expect AQI to increase. The false color image above also shows a decrease in air quality in the band that matches the coordinates of the Thomas fire. From both these plots, we can see that the air quality was significantly effected by the Thomas Fire. This change is evident in both numeric data reporting AQI levels, as well as satellite data."
  },
  {
    "objectID": "posts/aqi-santa-barbara/air_quality_index-sb.html#about",
    "href": "posts/aqi-santa-barbara/air_quality_index-sb.html#about",
    "title": "Analyzing air quality after the Santa Barbara Thomas Fire",
    "section": "",
    "text": "The Thomas Fire caused extensive damage in both Ventura and Santa Barbara County in December of 2017. As with all fires, the decrease in air quality index is a a direct effect of wildfires. This project will explore how the air quality index was effected by the Thomas fire. We will look into both numeric AQI data of santa barbara, as well as a collection of bands from the Landsat 8 Satellite. Looking into this different areas will allow us to further investigate how much aqi was affected, and from what bands the effect could be seen.\n\n\n\n\nfetch AQO data, landsat sattelite data, and california fire perimeter shapefile from an online repository\nClean data with consistent crs’ for plotting purposes\nTime series analysis with aqi data\nVisualize landsat raster data with false and true color images\nCompare landsat raster with Thomas fire perimeter"
  },
  {
    "objectID": "posts/aqi-santa-barbara/air_quality_index-sb.html#data",
    "href": "posts/aqi-santa-barbara/air_quality_index-sb.html#data",
    "title": "Analyzing air quality after the Santa Barbara Thomas Fire",
    "section": "",
    "text": "A simplified collection of bands (red, green, blue, near-infrared and shortwave infrared) from the Landsat Collection 2 Level-2 atmosperically corrected surface reflectance data, collected by the Landsat 8 satellite. The data was accessed and pre-processed in the Microsoft Planetary Computer to remove data outside land and coarsen the spatial resolution. Data can be found in the Landsat Collection in the Microsoft Planetary Computer here.\n\n\n\nA shapefile of fire perimeters in California during 2017. The complete file can be accessed in the CA state geoportal.\n\n\n\nA daily reporting of AQI by county for 2017 and 2018. Data reports an AQI value, approximately every 3 -7 days, for counties in every state. Also reports the category for AQI, i.e. “good”. Data reports an AQI value, approximately every 3 -7 days, for 804 counties across every state (as well as Country of Mexico, Puerto Rico, and Virgin Islands).\n\n\n\nCalifornia State Geoportal. 2023. California Fire Perimeters (All). https://gis.data.ca.gov/datasets/CALFIRE-Forestry::california-fire-perimeters-all-1/about (Accessed 2023-11-28).\nNASA, USGS. 2023. Landsat Collection 2 Level-2. Hosted by Microsoft Planetary Computer.https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2 (Accessed 2023-11-22).\nUnited States Environmental Protection Agency. Daily AQI by County.https://aqs.epa.gov/aqsweb/airdata/download_files.html#AQI (Accessed 2023-11-22)."
  },
  {
    "objectID": "posts/aqi-santa-barbara/air_quality_index-sb.html#analysis",
    "href": "posts/aqi-santa-barbara/air_quality_index-sb.html#analysis",
    "title": "Analyzing air quality after the Santa Barbara Thomas Fire",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport os\nimport matplotlib.patches as mpatches # for creating legends\nimport xarray as xr\nimport rioxarray as rioxr\nimport geopandas as gpd\nfrom rasterio.features import rasterize #for reasterizing polygons\nimport matplotlib.lines as mlines\n\n\n\n\nCode\n#read in aqi data for 2017 and 2018\naqi_17 = pd.read_csv('https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2017.zip') # 2017 data\naqi_18 = pd.read_csv('https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2018.zip') # 2018 data\n\n# read in shapefile of califronia fire/ perimters\nca_fire = gpd.read_file('data/California_Fire_Perimeters_2017/California_Fire_Perimeters_2017.shp') \n\n#read in landsat data\nlandsat_fp = os.path.join(os.getcwd(), 'data','landsat8-2018-01-26-sb-simplified.nc') \nlandsat = rioxr.open_rasterio(landsat_fp)\n\n\n\n\n\nSelect data pertaining to area of interest (Santa Barbara) and merge data for our years of interest ( 2017 and 2018)\n\n\nCode\n # concatenate 2017 and 2018 data using concat to get one dataframe for all aqi data\naqi = pd.concat([aqi_17, aqi_18])\n\n#only select data in Santa barbara since that is where Thomas fire was \naqi_sb = aqi[aqi[\"county Name\"] == \"Santa Barbara\"].drop(columns = ['State Name','county Name','State Code','County Code']) #select Santa barbara and drop unnecessary columns\n\n#only select thomas fire perimeter\nthomas_fire = ca_fire[ca_fire.FIRE_NAME == \"THOMAS\"]\n\n#check to make sure there is now only one unqiue value in thomas _fire\nprint(thomas_fire.FIRE_NAME .nunique())\n\n\n1\n\n\n\n\nCode\naqi.head()\n\n\n\n\n\n\n\n\n\nState Name\ncounty Name\nState Code\nCounty Code\nDate\nAQI\nCategory\nDefining Parameter\nDefining Site\nNumber of Sites Reporting\n\n\n\n\n0\nAlabama\nBaldwin\n1\n3\n2017-01-01\n21\nGood\nPM2.5\n01-003-0010\n1\n\n\n1\nAlabama\nBaldwin\n1\n3\n2017-01-04\n22\nGood\nPM2.5\n01-003-0010\n1\n\n\n2\nAlabama\nBaldwin\n1\n3\n2017-01-10\n19\nGood\nPM2.5\n01-003-0010\n1\n\n\n3\nAlabama\nBaldwin\n1\n3\n2017-01-13\n30\nGood\nPM2.5\n01-003-0010\n1\n\n\n4\nAlabama\nBaldwin\n1\n3\n2017-01-16\n16\nGood\nPM2.5\n01-003-0010\n1\n\n\n\n\n\n\n\n\n\n\nExplore information on all three datasets to understand data setup\n\n\n\n\nCode\nprint(\"Number of unique values : \\n\", aqi_sb.nunique(), \"\\n\") # check number of unique values to get idea on variety of data\nprint(\"Column datatypes:\\n \", aqi_sb.dtypes, \"\\n\")  # check data types of variables for future reference\nprint(\"Shape of data:\", aqi_sb.shape) # check shape of data to see how many columns and observations there are\n\n\nNumber of unique values : \n Date                         730\nAQI                           75\nCategory                       5\nDefining Parameter             3\nDefining Site                 12\nNumber of Sites Reporting      4\ndtype: int64 \n\nColumn datatypes:\n  Date                         object\nAQI                           int64\nCategory                     object\nDefining Parameter           object\nDefining Site                object\nNumber of Sites Reporting     int64\ndtype: object \n\nShape of data: (730, 6)\n\n\n\n\n\n\n\nCode\nprint(\"Column names: \\n\", ca_fire.columns) # check column names to see what the data contains\nprint(\"Shape of data:\", ca_fire.shape,\"\\n\") # check shape of data to see how many columns and observations there are\nprint(\"Column datatypes:\\n \", ca_fire.dtypes, \"\\n\")  # check data types of variables for future reference\n\n\nColumn names: \n Index(['index', 'OBJECTID', 'YEAR_', 'STATE', 'AGENCY', 'UNIT_ID', 'FIRE_NAME',\n       'INC_NUM', 'ALARM_DATE', 'CONT_DATE', 'CAUSE', 'C_METHOD', 'OBJECTIVE',\n       'GIS_ACRES', 'COMMENTS', 'COMPLEX_NA', 'COMPLEX_IN', 'IRWINID',\n       'FIRE_NUM', 'DECADES', 'SHAPE_Leng', 'SHAPE_Area', 'geometry'],\n      dtype='object')\nShape of data: (608, 23) \n\nColumn datatypes:\n  index            int64\nOBJECTID         int64\nYEAR_           object\nSTATE           object\nAGENCY          object\nUNIT_ID         object\nFIRE_NAME       object\nINC_NUM         object\nALARM_DATE      object\nCONT_DATE       object\nCAUSE          float64\nC_METHOD       float64\nOBJECTIVE      float64\nGIS_ACRES      float64\nCOMMENTS        object\nCOMPLEX_NA      object\nCOMPLEX_IN      object\nIRWINID         object\nFIRE_NUM        object\nDECADES         object\nSHAPE_Leng     float64\nSHAPE_Area     float64\ngeometry      geometry\ndtype: object \n\n\n\n\n\n\n\n\nCode\nprint(\"The dimmensions of the landsat data are:\\n\", landsat.dims,\"\\n\") # check file dimmensions\nprint(\"The coordinates of the landsat data are:\\n\", landsat.coords,\"\\n\") # check coordinates of shapefile\nprint(\"The values of the landsat data are:\\n\",  landsat.values, \"\\n\") # values of xarray\nprint(\"The crs of the landsat data is:\", landsat.rio.crs) # check crs\n\n\nThe dimmensions of the landsat data are:\n Frozen({'y': 731, 'x': 870, 'band': 1}) \n\nThe coordinates of the landsat data are:\n Coordinates:\n  * y            (y) float64 3.952e+06 3.952e+06 ... 3.756e+06 3.755e+06\n  * x            (x) float64 1.213e+05 1.216e+05 ... 3.557e+05 3.559e+05\n  * band         (band) int64 1\n    spatial_ref  int64 0 \n\nThe values of the landsat data are:\n &lt;bound method Mapping.values of &lt;xarray.Dataset&gt;\nDimensions:      (y: 731, x: 870, band: 1)\nCoordinates:\n  * y            (y) float64 3.952e+06 3.952e+06 ... 3.756e+06 3.755e+06\n  * x            (x) float64 1.213e+05 1.216e+05 ... 3.557e+05 3.559e+05\n  * band         (band) int64 1\n    spatial_ref  int64 0\nData variables:\n    red          (band, y, x) float64 ...\n    green        (band, y, x) float64 ...\n    blue         (band, y, x) float64 ...\n    nir08        (band, y, x) float64 ...\n    swir22       (band, y, x) float64 ...&gt; \n\nThe crs of the landsat data is: EPSG:32611\n\n\n\n\n\n\n\n\nAdjust column names, make data a datetime object, and set index of dataset to be data for future plotting purposes\n\n\nCode\naqi_sb.columns = aqi_sb.columns.str.lower() #make column names lower case\naqi_sb.columns = aqi_sb.columns.str.replace(' ','_') # reassign column names by substituting an underscore for a space\naqi_sb.date = pd.to_datetime(aqi_sb.date) # update data column to datetime\naqi_sb = aqi_sb.set_index('date') # change index to be date\n\n\n\n\n\nDrop Landsat band so we are able to plot landsat raster\n\n\nCode\n# drop band dimension to make data 2D\nlandsat_2d = landsat.squeeze().drop('band')\nlandsat_2d # check to make sure band is no longer included\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:      (y: 731, x: 870)\nCoordinates:\n  * y            (y) float64 3.952e+06 3.952e+06 ... 3.756e+06 3.755e+06\n  * x            (x) float64 1.213e+05 1.216e+05 ... 3.557e+05 3.559e+05\n    spatial_ref  int64 0\nData variables:\n    red          (y, x) float64 ...\n    green        (y, x) float64 ...\n    blue         (y, x) float64 ...\n    nir08        (y, x) float64 ...\n    swir22       (y, x) float64 ...xarray.DatasetDimensions:y: 731x: 870Coordinates: (3)y(y)float643.952e+06 3.952e+06 ... 3.755e+06axis :Ycrs :EPSG:32611long_name :y coordinate of projectionresolution :-30standard_name :projection_y_coordinateunits :metre_FillValue :nanarray([3952395., 3952125., 3951855., ..., 3755835., 3755565., 3755295.])x(x)float641.213e+05 1.216e+05 ... 3.559e+05axis :Xcrs :EPSG:32611long_name :x coordinate of projectionresolution :30standard_name :projection_x_coordinateunits :metre_FillValue :nanarray([121305., 121575., 121845., ..., 355395., 355665., 355935.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32611\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 11Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32611\"]]GeoTransform :121170.0 270.0 0.0 3952530.0 0.0 -270.0array(0)Data variables: (5)red(y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]green(y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]blue(y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]nir08(y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]swir22(y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]Attributes: (0)\n\n\n\n\n\nMake sure crs of thomas fire shapefile is same as the crs for the landsat raster, change crs if not the same.\n\n\nCode\nthomas_fire= thomas_fire.to_crs(landsat_2d.rio.crs) # change crs of thomas fire to match landsat crs\n\n\n\n\nCode\n#check to make sure crs' match/\nthomas_fire.crs == landsat.rio.crs\n\n\nTrue\n\n\n\n\n\n\n\n\nPlot five day average of aqi and daily aqi to see different trends\n\n\nCode\naqi_sb['five_day_average'] = aqi_sb.aqi.rolling('5D').mean() # create new column called 'five_day_average' in aqi_sb data frame that contains a 5 day rolling window\ncolors = {'aqi': '#964B00', # pick dark brown for aqi\n         'five_day_average': 'cyan'}  # pick cyan for five day average\naqi_sb.plot(use_index = True, # include index as x axis\n       y = ['aqi','five_day_average'], # plot aqi and five_day_average\n       color = colors,\n           title = 'AQI and Five day AQI Average in Santa Barbara County from Jan 2017 to December 2018') # have colors be pre defined colors dictionary, add title\n\n\n&lt;AxesSubplot:title={'center':'AQI and Five day AQI Average in Santa Barbara County from Jan 2017 to December 2018'}, xlabel='date'&gt;\n\n\n\n\n\n\n\n\nCreate a true color image by plotting the red, green, and blue variables.\n\n\nCode\nfig, ax = plt.subplots(figsize = (12,8)) # set figure size and axis\nax.axis('off') # turn x and y axis off\n\n# -------------------------\nlandsat_2d[['red', 'green', 'blue']].to_array().plot.imshow(ax = ax,robust = True)\nthomas_fire.plot(ax = ax,color = 'None', edgecolor = 'red') # plot california map with black outline \nthomas_line = mlines.Line2D([], [], color='red', marker='_', # make legend have triangle #update legend name and marker\n                          markersize=15, label='Thomas Fire Perimeter', linestyle = 'None')\n\n#---------------------------\n\n# create legend\nax.legend(handles = [thomas_line], frameon = True, loc = 'upper right') # add legend in upper right corner\n\n#---------------------------\nplt.title(\"True color image\")\n\nplt.show()  # show plot\n\n\n\n\n\nCreate a false color image by plotting the short-wave infrared (swir22), near-infrared, and red variables.\n\n\nCode\n#select short wave, near infrared, and red variables, make an array and plot with robust = True\nlandsat_2d[[ 'swir22','nir08', 'red']].to_array().plot.imshow(size = 4, robust = True)\nplt.title(\"False color image\")\n\n\nText(0.5, 1.0, 'False color image')\n\n\n\n\n\nThis image looks good, but we want to look closer at the Thomas fire region. We can zoom in on the Thomas fire region to see that the landsat imaage picked up for the region using a false color image.\n\n\nCode\nfig, ax = plt.subplots(figsize = (12,8)) # set figure size and axis\nax.axis('off') # turn x and y axis off\n\n# -------------------------\nlandsat_2d[['swir22','nir08', 'red']].to_array().plot.imshow(ax = ax,robust = True)\nthomas_fire.plot(ax = ax,color = 'None', edgecolor = 'red') # plot california map with red outline \nthomas_line = mlines.Line2D([], [], color='red', marker='_', #make legend symbol be a line and change color to red, add label\n                          markersize=15, label='Thomas Fire Perimeter', linestyle = 'None')\n\n#---------------------------\n\n# create legend\nax.legend(handles = [thomas_line], frameon = True, loc = 'upper right') # add legend in upper right corner\n#---------------------------\nplt.title(\"False color image with Thomas Fire Perimeter\")\n\nplt.show()  # show plot\n\n\n\n\n\n\n\n\n\nThe AQI time series graph and landsat image above both display increased AQI levels during the Thomas fire. The time series graph has a spike in AQI during December of 2017/ January 2018. This corresponds to the time of the Thomas Fire, when we would expect AQI to increase. The false color image above also shows a decrease in air quality in the band that matches the coordinates of the Thomas fire. From both these plots, we can see that the air quality was significantly effected by the Thomas Fire. This change is evident in both numeric data reporting AQI levels, as well as satellite data."
  },
  {
    "objectID": "posts/salinity_analysis/index.html",
    "href": "posts/salinity_analysis/index.html",
    "title": "Salinity in the CCE",
    "section": "",
    "text": "Image from NASA"
  },
  {
    "objectID": "posts/salinity_analysis/index.html#footnotes",
    "href": "posts/salinity_analysis/index.html#footnotes",
    "title": "Salinity in the CCE",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUniversity of North Florida. (2023, July 13). Salinity changes threatening marine ecosystems, new study shows. Phys.org. https://phys.org/news/2023-07-salinity-threatening-marine-ecosystems.html↩︎\nOlofsson, P., & Virts, K. (2023, March 7). Addressing needs for sea surface salinity data. Earthdata. NASA. https://www.earthdata.nasa.gov/news/updated-smap-sss-data-products↩︎\nCalifornia Cooperative Oceanic Fisheries Investigations. (2020). CalCOFI Data Collection 2020. Scripps Institution of Oceanography. https://calcofi.org/ccdata/datasets/2020.html↩︎"
  },
  {
    "objectID": "posts/blackout_houston_post/index.html",
    "href": "posts/blackout_houston_post/index.html",
    "title": "Quantifying Houston Blackouts after Texas’ 2021 Winter Storm",
    "section": "",
    "text": "“In February 2021, the state of Texas suffered a major power crisis, which came about as a result of three severe winter storms sweeping across the United States on February 10–11, 13–17, and 15–20.”1. Because Texas is on its own power grid, it was not able to easily get access to power from other states.Therefore, it felt the effects of this winter storm at a much more servere rate than neighboring states that were experiencing the same exact storm. Some buildings lost power for five days straight, affecting a total of 4 million Texans."
  },
  {
    "objectID": "posts/blackout_houston_post/index.html#background",
    "href": "posts/blackout_houston_post/index.html#background",
    "title": "Quantifying Houston Blackouts after Texas’ 2021 Winter Storm",
    "section": "",
    "text": "“In February 2021, the state of Texas suffered a major power crisis, which came about as a result of three severe winter storms sweeping across the United States on February 10–11, 13–17, and 15–20.”1. Because Texas is on its own power grid, it was not able to easily get access to power from other states.Therefore, it felt the effects of this winter storm at a much more servere rate than neighboring states that were experiencing the same exact storm. Some buildings lost power for five days straight, affecting a total of 4 million Texans."
  },
  {
    "objectID": "posts/blackout_houston_post/index.html#overview",
    "href": "posts/blackout_houston_post/index.html#overview",
    "title": "Quantifying Houston Blackouts after Texas’ 2021 Winter Storm",
    "section": "Overview",
    "text": "Overview\nI will utilize a few different datasets to attempt to determine just how many buildings lost power in this storm. To classify the number of houses, I am going to use satelite data from before/ during the outage. I will focus specifically on Houston. I will first estimate the number of homes in Houston that lost power due to the storm, and then will explore if socioeconomic factors are predictors of communities recovery from a power outage."
  },
  {
    "objectID": "posts/blackout_houston_post/index.html#data",
    "href": "posts/blackout_houston_post/index.html#data",
    "title": "Quantifying Houston Blackouts after Texas’ 2021 Winter Storm",
    "section": "Data",
    "text": "Data\n\nNight lights data\nI will sse NASA’s Worldview to explore the data around the day of the storm. There are several days with too much cloud cover to be useful, but 2021-02-07 and 2021-02-16 provide two clear, contrasting images to visualize the extent of the power outage in Texas. Additionally, I will utilize VIIRS data, which is distributed through NASA’s Level-1 and Atmospheric Archive & Distribution System Distributed Active Archive Center (LAADS DAAC). Many NASA Earth data products are distributed in 10x10 degree tiles in sinusoidal equal-area projection. Tiles are identified by their horizontal and vertical position in the grid. Datasets that were previously prepped and cleaned will be utilized.\n\nVNP46A1.A2021038.h08v05.001.2021039064328.h5.tif: tile h08v05, collected on 2021-02-07\nVNP46A1.A2021038.h08v06.001.2021039064329.h5.tif: tile h08v06, collected on 2021-02-07\nVNP46A1.A2021047.h08v05.001.2021048091106.h5.tif: tile h08v05, collected on 2021-02-16\nVNP46A1.A2021047.h08v06.001.2021048091105.h5.tif: tile h08v06, collected on 2021-02-16\n\n\n\nRoad data\nTo prevent misrepresenting roads as building lights, I will utilize this road dataset. OpenStreetMap (OSM) is a collaborative project which creates publicly available geographic data of the world. Ingesting this data into a database where it can be subsetted and processed is a large undertaking. Fortunately, third party companies redistribute OSM data. I will use a prepared Geopackage containing just the subset of roads that intersect the Houston metropolitan area that got the data from Geofabrik’s download sites.\n\ngis_osm_roads_free_1.gpkg\n\n\n\nHouse data\nI can also obtain building data from OpenStreetMap to quantiy where the houses in Houston are. I will once again be using a preloaded package containing only houses in the Houston metropolitan area, with data coming from Geofabrick.\n\ngis_osm_buildings_a_free_1.gpkg\n\n\n\nSocioeconomic data\nSocioeconomic information for every home is not readily available, so instead I obtained data from the U.S. Census Bureau’s American Community Survey for census tracts in 2019. The folder ACS_2019_5YR_TRACT_48.gdb is an ArcGIS “file geodatabase”, a multi-file proprietary format that’s roughly analogous to a GeoPackage file. Each layer of the geodatabase contains a subset of the fields documents in the ACS metadata. The geodatabase contains a layer holding the geometry information, separate from the layers holding the ACS attributes.\nNow that the data have been reviewed, here is a preview into the steps I will be taking!\n- Find locations that experienced a blackout of more than 200 nW cm-2sr-1\n- Exclude highways that are within 200 meters of homes\n- Find number of homes that experienced a blackout with given the two conditions mentioned above\n- Compare blackout areas with socioecnomic factors to see if there is any correlation"
  },
  {
    "objectID": "posts/blackout_houston_post/index.html#analysis",
    "href": "posts/blackout_houston_post/index.html#analysis",
    "title": "Quantifying Houston Blackouts after Texas’ 2021 Winter Storm",
    "section": "Analysis",
    "text": "Analysis\n\nFind locations of blackouts\nLet’s first explore the effect the storm had on all of Houston in terms of outages. We will classify any drop of more than 200 nW cm-2sr-1 to be considered a blackout.\n\n\n\n\n\nThis image shows the areas that experienced a blackout of more than 200 nW cm-2sr-1 in Houston between February 7th and February 16th. These points are in the general vacinity of Houston and includes far beyond just buildings. This raster is a starting point and we will use it to select homes as a next step.\n\n\nExclude highways from blackout mask\nSince we want to exclude highways so that we are not including them as houses to be counted, we can create a buffer of 200 meters from all highways and then disjoin the buffer and the previous vectorized blackout data to exclude the buffered area. Therefore, we will only be including houses that are at least 200 meters away from a highway. As a reminder, we are going to use EPSG:3083 (NAD83 / Texas Centric Albers Equal Area) for all areas of interest in this analysis.\nSetting a buffer of 200 meters around houses means that any house that is within 200 meters of a highway will not be included in our analysis. This is because highways give off a lot of light from cars on the road. We want to be sure that the light we are picking up when trying to quantify the number of houses that experienced a blackout is starting with homes and not highways.\n\n\nFind homes impacted by blackouts\nI will now account for the homes within the new selected area. I can do so by filtering to areas in our most recent highway_mask vector. Doing so will give me buildings in Houston that experienced a drop of more than 200 nW cm-2sr-1 in areas that are farther than 200 meters from a highway. The number of houses can be obtained simply by counting the number of rows in my newly filtered buildings dataframe.\n\n\n[1] \"157411 houses experienced a blackout in Houston, Texas.\"\n\n\n\n\nInvestigate socioeconomic factors\nI will now utilize the acs data to see if there is any relationship between blackout areas and different socioecnomic factors. Specifically, I looked to see if the outages were at all related to the median income. I will classify the NA buiding value ( buildings that did not experienced a drop of 200 nW cm-2sr-1) as No Blackout. From there, I can check to see if there are any differences in median values among tracts with a blackout versus tracts without a blackout. I’ll first create a map to visual median incomes across the county of Houston. I will add a centroid for each census group that experienced a blackout.\n\n\n\n\n\n\n\nIs median income related to outages?\n\n\n\n\n\nI created a faceted histogram plot to look into the distribution of median income for each blackout status. The map allowed us to geographically see the areas of Houston that experienced a blackout, but creating a histogram and printing out some summary statistics will let us determine a bit more specifically if any obvious median income trends can be deduced. The histogram above shows far more buidlings experienced a blackout rather than not, but the median incomes for the two groups don’t look too different. Both histograms are skewed right and the mean income for both groups looks to be pretty similar. Let’s look at the average median income for both groups to get a better idea on how much they differ.\n\n\n[1] \"The mean Median income for census' that experienced a blackout is $71244.88\"\n\n\n[1] \"The mean Median income for census' that did not experience a blackout is $73911.55\"\n\n\nThe average median income for houses that did not experience a blackout is actually higher than those that did. We can dive into this relationship a bit more by creating a hypothesis test to see if the difference between the two groups is significant.\nAllow our hypotheses to be as follows: \\[H_{0}: \\mu_{Blackout} - \\mu_{No Blackout} = 0\\] \\[H_{A}: \\mu_{Blackout} - \\mu_{No Blackout} \\neq 0\\]\n\n\n[1] \"p value: 1.601\"\n\n\nSince the p−value= 1.6 &gt; 0.05, we fail to reject the null that there is no difference between the average median income for tracts that experienced a blackout and tracts that did not experience a blackout. There is not a statistically significant difference (at the 5% significance level) in average median income levels across tracts that experienced a blackout and tracts that did not experience a blackout.\n\n\nResults and Limitations\nThe map, histogram, and hypothesis testing above all point to median income not playing a role in predicting whether a house experienced a blackout or not. This study did not consider the fact that there may be more homes in lower income census tracts than higher income census tracts, and therefore we cannot make accurate findings from points on a map. The exclusion of homes within 200 meters could also disproportionately exclude lower median income homes. The study also did not account for the fact that moon could have been providing a lot of light on Febuary 7th, and we could have misclassified houses to begin with. Finally, the study only looked at median income and not any other socioecnomic factors. It could be that there is a different socioeconomic factor that is much more correlated with experiencing a blackout.\nThis goal of this investigation was to become more familiar with spatial data and attempt to roughly quantify homes that experienced a blackout with satelittle data alone. The results of this analysis are not final and should not be cited.\n\nTwo years later, Texas has still not recovered from the 2021 storm\nTwo years and billions of dollars later, Texas has somewhat recovered from the damage done by the 2021 storm and power outages, but it is not back to where it stood prior to the storm, especially in terms of the energy grid. Work has been done to make more energy vailable and make the grid better suited for extreme weather, but reporters claim that if an identical storm to that of Febuary 2021 were to hit right now, the energy grid would once again fail. Policy makers hope to bring about changes that would change Texas’ grid. The policy changes proposed so far include favoring fossil fuel producers and big energy companies2. While the status of the Texas grid has not changed, Texans have become more prepared for future storms by purchasing generators and supplies to hold them through a storm/outage."
  },
  {
    "objectID": "posts/blackout_houston_post/index.html#footnotes",
    "href": "posts/blackout_houston_post/index.html#footnotes",
    "title": "Quantifying Houston Blackouts after Texas’ 2021 Winter Storm",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWikipedia. 2021. “2021 Texas power crisis.” Last modified October 2, 2021. https://en.wikipedia.org/wiki/2021_Texas_power_crisis.↩︎\nKUT.org. 2023.”Two years later: The 2021 blackout still shapes what it means to live in Texas.” https://www.kut.org/energy-environment/2023-02-17/two-years-later-the-2021-blackout-still-shapes-what-it-means-to-live-in-texas↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Annie Adams",
    "section": "",
    "text": "Hi there! Welcome to my website!\nMy name is Annie Adams! I am a current Master of Environmental Data Science candidate at the University of California, Santa Barbara. I am extremely motivated by environmental data and how it can assist in reducing the climate crisis. When I am not behind a computer, I love to hike, cycle, explore new countries, and bikepack! Check out the rest of my website to learn more about me and see what I have been up to!"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "Seattle Bike Trends\n\n\n\n\n\n\n\nData Visualization\n\n\nR\n\n\ninfographic\n\n\n\n\n\n\n\n\n\n\n\nMar 2, 2024\n\n\nAnnie Adams\n\n\n\n\n\n\n  \n\n\n\n\nSalinity in the CCE\n\n\n\n\n\n\n\nOcean Analysis\n\n\nGeospatial\n\n\nLinear Regression\n\n\nKriging\n\n\n\n\nExploring Salinity levels in the California Coastal Ecosystem\n\n\n\n\n\n\nDec 12, 2023\n\n\nAnnie Adams\n\n\n\n\n\n\n  \n\n\n\n\nQuantifying Houston Blackouts after Texas’ 2021 Winter Storm\n\n\n\n\n\n\n\nGeospatial\n\n\nR\n\n\nRaster data\n\n\nVector data\n\n\n\n\nDetermining just how many houses in Houston, Texas lost power due to the 2021 storm\n\n\n\n\n\n\nDec 12, 2023\n\n\nAnnie Adams\n\n\n\n\n\n\n  \n\n\n\n\nAnalyzing air quality after the Santa Barbara Thomas Fire\n\n\n\n\n\n\n\nPython\n\n\nGIS\n\n\nClimate\n\n\nMEDS\n\n\n\n\nImplement both tabular and raster data to see changes in AQI before and after the Santa Barbara Thomas Fire in 2017.\n\n\n\n\n\n\nDec 6, 2023\n\n\nAnnie Adams\n\n\n\n\n\n\nNo matching items"
  }
]